%\VignetteIndexEntry{Subrank}
%\VignettePackage{subrank}
\documentclass{article}
\usepackage{multirow,natbib,lmodern,amsmath,amssymb,graphicx,a4wide}
\usepackage[pdftex]{hyperref}

%               ---- Probabilistes
\newcommand{\esp}[1]{\mathbb{E} \left( #1 \right)} % esperance
\newcommand{\esploi}[2]{\mathbb{E}_{#2} \left( #1 \right)} % esperance
\newcommand{\var}[1]{\mathbb{V} \left( #1 \right)} % esperance
\newcommand{\varloi}[2]{\mathbb{V}_{#2} \left( #1 \right)} % esperance
\newcommand{\proba}[1]{\mathbb{P} \left( #1 \right)} % probabilite
\newcommand{\cov}[1]{\mathbb{C}ov \left( #1 \right)} % covariance
\newcommand{\ind}[1]{\mathbf{1}_{\left\{ #1 \right\}}}
\newcommand{\Ind}[1]{\mathbf{1}{\left\{ #1 \right\}}}
\newcommand{\card}[1]{\left| #1 \right|}
%               ---- Non Probabilistes (notations standard)
\newcommand{\ent}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ens}[1]{\{1,\cdots,#1\}}
%               ---- Nombres
\newcommand{\nb}[2]{#1e{#2}} % nombre ``info''
\newcommand{\nbt}[2]{#1 \cdot 10^{#2}} % nombre litt\'eraire
\newcommand{\ordinal}[1]{$#1^{\textrm{th}}$}
%               ---- Mise en forme de texte (conception logique !)
\newcommand{\important}[1]{\textbf{#1}}
\newcommand{\gui}[1]{"{#1}"}
\newcommand{\cod}[1]{\texttt{#1}}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\newlength{\taillecarre}
\setlength{\taillecarre}{0.5\textwidth}


\begin{document}


\title{Package \cod{subrank}: estimating copula density using ranks and sub-sampling}

\author{\href{mailto:jeromepcollet@gmail.com}{J\'er\^ome Collet}}
\date{March 31, 2016}

\maketitle


\section{Introduction}
This package estimates the copula density of a $d$-dimensional random variable, without parametric assumptions, using ranks and subsampling. The main feature of this method is that simulation studies show a low sensitivity to dimension, on realistic cases.\\
This vignette provides:
\begin{itemize}
  \item A description of the estimation method, in different ways.
  \item A description of the regression derived from the estimation method. In the case of regression, it is possible to estimate density on-the-fly, so computational burden is largely reduced.
  \item A convergence proof.
  \item Some hints on convergence speed. In the simple useless case of independence, we prove the convergence speed is the same as for kernel density estimation. In more structured cases, some numerical simulations show the impact of dimension is much smaller than for a kernel estimation.
\end{itemize}
It is usual to use ranks to estimate copulas~\citep{jdf},  or test uniformity of the copula~\citep{hoeffding,Kojadinovic20091137}, since it gives straightforwardly uniform marginals. Nevertheless, with usual methods, we do not use all the features of ranks. They do not only have uniform marginals: their marginals are \important{exactly} uniform on~$\ens{n}$ for \important{any} sample, and \important{any} dimension of a multidimensional variable. We have to use this regularization to reduce sampling errors. Furthermore, the smaller the sample is, the stronger the regularization is, so it is compulsory to sub-sample to adjust the strength of the regularization.\\
Sub-sampling is now widely used and studied, so we only cite the pioneering work~\citep{breiman1996bagging} and the survey~\citep{buhlmann2012bagging}. These methods are applied to \important{estimators}, so the theoretical results in these papers do not exactly fit our goal. Nevertheless, the main conclusion is that sub-sampling related methods are smoothing methods: their efficiency increases when applied to discontinuous, non-linear functions. Yet, the ranking of the observations is highly discontinuous and non-linear, so sub-sampling is relevant.
\subsection{Implementation} This package is partly written in~\texttt{C} (parallelized with~\texttt{Open-MP}~\citep{openmp}), for reasons of efficiency. The random number generator is described in~\citep{randomkit}.



\section{Description of the estimation method}
Given a sample of size~$n$, for a given sub-sample size~$m<n$, one draws many sub-samples, without replacement. For each sub-sample and each observation, one obtains a vector of ranks (in set~$\ens{m}^d$). For each point~$\mathbf{r}$ of~$\ens{m}^d$, we propose to count the sub-samples where ~$\mathbf{r}$ appears (where there is an observation of which $d$~ranks are the point $\mathbf{r}$). This counting is the estimator we propose, it converges in some ways to the copula density. We give in the following a formal description.
\subsection{Mathematical formulation}
We study a random variable~$\mathbf{X}=(X_1, \cdots, X_d)$ in~$\mathbb{R}^d$, with copula~$c(\mathbf{u})=c(u_1,\cdots,u_d)$; its marginals are assumed to be continuous.\\
We define
\begin{itemize}
  \item~$\mathcal{S}$ a sample of~$\mathbf{X}$, $\mathbf{x}$ an element of~$\mathcal{S}$,
  \item $\mathbf{R}\left(\mathbf{x},\mathcal{S}\right)$ the $d$-uple of the ranks of the components of~$\mathbf{x}$ in sample~$\mathcal{S}$.
\end{itemize}
This allows defining a random variable~$\mathbf{B}_{\bullet}(\mathcal{S})$: it is an array, filled with~0s and~1s, $d$~dimensional, each dimension being indexed from~1 to~$\card{\mathcal{S}}$ (where~$\card{}$ stands for cardinal). For a $d$-uple of ranks~$\mathbf{r} \in \ens{\card{\mathcal{S}}}^d$, we define
\begin{gather*}
  \mathbf{B}_{\mathbf{r}}(\mathcal{S})=\Ind
  {\exists \mathbf{x} \in \mathcal{S}\ | \ \mathbf{R}\left(\mathbf{x},\mathcal{S}\right)=\mathbf{r}}
.\end{gather*}
In other words, $\mathbf{B}_{\mathbf{r}}(\mathcal{S})$ is equal to~1 if and only if~$\mathbf{r}$ is equal to~$\mathbf{R}\left(\mathbf{x},\mathcal{S}\right)$ for a~$\mathbf{x}$ in~$\mathcal{S}$.\\
The object we want to estimate is
\begin{gather*}
  P(\card{\mathcal{S}},\mathbf{r},c)=
  \frac{1}{\card{\mathcal{S}}}\esp{ \mathbf{B}_{\mathbf{r}}(\mathcal{S})}
,\end{gather*}
because we will show that this discrete array tends to the density copula~$c$. Let us note that dividing by~$\card{\mathcal{S}}$ makes the sum of~$P$ equal to~1, as $\forall \mathbf{r}, \forall \mathbf{X}, \sum_{\mathbf{r}}{ \mathbf{B}_{\mathbf{r}}(\mathcal{S})}=\card{\mathcal{S}}$.\\
In a practical setting, we have an $n$-sample~$\mathcal{T}$, we choose a sub-sample size~$m<n$, and we estimate~$P(m,\mathbf{r},c)$ using the following $U$-statistic:
\begin{gather*}
  \hat{P}_n(m,\mathbf{r},\mathbf{X}) = \frac{1}{m\binom{n}{m}} \sum_{\mathcal{S} \subset \mathcal{T},\card{\mathcal{S}}=m}{ \mathbf{B}_{\mathbf{r}}(\mathcal{S})}
.\end{gather*}

\paragraph*{Remarks}
\begin{itemize}
  \item If~$m=d=2$, counting the sub-samples reaching a vector of ranks is very similar to the Kendall's~$\tau$ computation. In such a case, $\ens{m}^d=\{(1,1),(1,2),(2,1),(2,2)\}$. A pair of concordant (resp. discordant) observations generate points~$(1,1)$ and~$(2,2)$ (resp. $(1,2)$ and~$(2,1)$), so we have~$\hat{\tau}=2(\hat{P}(2,(1,1),\mathbf{X})-\hat{P}(2,(1,2),\mathbf{X}))$.
  \item A simple example, in 2~dimensions, is the case~$X_2=f(X_1)$ with $f$~strictly increasing. Then the only weighted points of~$\ens{m}^2$ are on the diagonal. On the other hand, if all components are independent, a symmetry argument gives a uniform distribution on~$\ens{m}^d$ (for $n$ infinite, the case of a finite~$n$ is studied in the following).
  \item Most of the times $\binom{n}{m}$ will be far too large, so we will not be able to draw all sub-samples. We will use a random sub-sampling to obtain an approximation of~$\hat{P}_n$.
\end{itemize}

\subsection{Graphical description of the estimation method}
The estimation method is described in figure~\ref{BDestimation}.
\begin{figure}
\begin{center}
\includegraphics[width=0.84\textwidth, height=0.28\textwidth]{zetapeE3.pdf}
\includegraphics[width=0.84\textwidth, height=0.28\textwidth]{zetapeE6.pdf}
\includegraphics[width=0.84\textwidth, height=0.28\textwidth]{zetapeE9.pdf}
\includegraphics[width=0.84\textwidth, height=0.28\textwidth]{zetapeE12.pdf}
\includegraphics[width=0.84\textwidth, height=0.28\textwidth]{zetapeE15.pdf}
\caption{\label{BDestimation}\textbf{First steps of the estimation}: pictures on the left represent original data with points of the sub-sample in black; pictures in the center represent the sub-sample in the rank space; radii of circles on the right represent, for each rank vector, current sum of the hits.}
\end{center}
\end{figure}

\subsection{Small completely detailed example}
We propose a small completely detailed example. Table~\ref{exampledata} is a sample in~$\mathbb{R}^2$ (each observation is identified by a lowercase letter), we choose~$n=4$ and~$m=3$. Table~\ref{examplecalcul} summarizes the computations.
\begin{table}[ht!]
\begin{center}
\caption{\label{exampledata}\textbf{Example data}: $R_X$ (resp~$R_Y$) stands for the rank in~$X$ (resp.~$Y$)}
\begin{tabular}{|c|c|c|c|c|}
\hline
Observation & $X$ & $Y$ & $R_X$ & $R_Y$\\\hline
 a &2.29&-0.97&4&1\\
 b &-1.2&-0.95&1&2\\
 c &-0.69&0.75&2&4\\
 d &-0.41&-0.12&3&3\\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[ht!]
\begin{center}
\caption{\label{examplecalcul}\textbf{Computation of~$\hat{P}_4(3,\mathbf{r},\mathbf{X})$ for the data of Table~\ref{exampledata}}: the sub-samples are named:~$A=\{a,b,c\},B=\{a,b,d\},C=\{a,c,d\},D=\{b,c,d\}$. For example, $bD$ in the first cell of the first line means that observation~$b$ in sub-sample~$B$ is the first one in~$X$ and in~$Y$, and~1/12 is the value of~$\hat{P}_4(3,(1,1),\mathbf{X})$, since~$12=3\times\binom{4}{3}$}
\begin{tabular}{|c|c|ccc|}
\hline
&&\multicolumn{3}{|c|}{Rank in~$X$}\\\hline
&&1&2&3\\\hline
\multirow{3}{*}{Rank in~$Y$}
& 1 & \{bD\}; 1/12 & $\varnothing$; 0/12 & \{aA,aB,aC\}; 3/12 \\
& 2 & \{bA,bB\}; 2/12 & \{dC\}; 1/12 & \{dD\}; 1/12 \\
& 3 & \{cC\}; 1/12 & \{cA,dB,cD\}; 3/12 & $\varnothing$; 0/12 \\
\hline
\end{tabular}\end{center}
\end{table}

\subsection{Use for regression}
Using Theorem~\ref{equivkernel}, it is possible to use the estimation~$\hat{P}_n$ to build a regression model. For a given value of some components of~$\mathbf{x}$, we know the conditional copula. Using this, we may simulate values of the unknown components.\\
More precisely, we have a training set of $n$~observations in dimension~$d$, and we use it to estimate~$\hat{P}_n$ (given a sub-sample size~$m$), and $d$ cumulative distribution functions. We also have a new observation~$\mathbf{x}$, with only $d'$ components known, we want to know the remaining components.\\
The first step is to use the estimated CDFs to compute the $F_i(x_i)$'s (for the $d'$~known components). Multiplying by~$m$ and taking the integer part gives a vector of ranks in~$\ens{m}^{d'}$. Conditionally to this vector, we choose randomly a vector of ranks in~$\ens{m}^{d-d'}$. Dividing by~$m$ gives the $F_i(x_i)$'s for the $d-d'$~unknown components of~$\mathbf{x}$. We may smooth this prediction, adding a beta-distributed noise, or use another distribution. Finally, we use estimated CDFs, and their inverse, to compute the unknown components of~$\mathbf{x}$.\
This result is obviously random, since the vector of ranks in~$\ens{m}^{d-d'}$ is randomly chosen. So, it is a simulation method to provide a probabilistic forecast.
\paragraph{Prediction on-the-fly} Doing so, computation time is very important, and memory requirements too. That is why we prefer to estimate the copula density on-the-fly, around the value of the known components. It reduces drastically the computation time and the memory requirements.\\
We consider the same training set, and the same incomplete observation. We draw a sub-sample with size~$m$ in the training set, we add the incomplete observation, and we replace each observation by its ranks. We look at whether, for each dimension, the rank of the new observation is the neighbor (differ by~1) of the rank of an observation of the sub-sample of the training set. In such a case, we use the value of this observation as a prediction (for the unknown components of the incomplete observation).\\
This operation is repeated many times, which gives a probabilistic forecast.



\section{Convergence to the copula}
\begin{definition}
Let~$m$ be a strictly positive integer, and~$r$ an integer between~1 and~$m+1$. Given~$m$~random variables $U_i$ with distribution~$U[0,1]$, the density~$K_{r,m}$ is the density of the random variable~$U \leadsto U[U_{(r-1)},U_{(r)}]$, where~$U_{(r)}$ denotes the value with rank~$r$, $U_{(0)}=0$ and~$U_{(m+1)}=1$.
\end{definition}
It is possible to express~$K_{r,m}$:
\begin{gather*}
  K_{r,m}(k)=\frac{m!}{(r-2)^+!(m-r)^+!} \times \int_{0<u<k<v<1}{ \frac{u^{(r-2)^+}(1-v)^{(m-r)^+}}{(v-u)}dudv}
,\end{gather*}
where~$(r-2)^+$ stands for the positive part of~$r-2$, in order to encompass all cases in one formula. These integrals seem to be difficult to write in a simpler form; even if~$r=1$, the integration is not easy.
% \begin{gather*}
%   K_{1,m}(k)=m \times \left(-\log(k)+
%       \sum_{i=1}^{m-1} { (-1)^i (1-x^i)\frac{\binom{m-1}{i}}{i} }
%     \right)\\
%   FK_{1,m}(k)=m \times \left(k(1-\log(k))+
%       \sum_{i=1}^{m-1} {(-1)^i (x-\frac{x^{i+1}}{i+1})\frac{\binom{m-1}{i}}{i} }
%     \right)\\
% .\end{gather*}
Nevertheless, it is possible to derive moments of this distribution:
%\\We know that for a uniform distribution~$U[a,b]$, we have~$\esp{X}=\frac{a+b}{2}$ and~$\esp{X^2}=\frac{a^2+ab+b^2}{3}$. So
%\begin{gather*}
%  \esploi{X}{K_{r,m}}=\frac{\frac{r-1}{m+1}+\frac{r}{m+1}}{2}=\frac{2r-1}{2(m+1)}\\
%  \varloi{X}{K_{r,m}}=\esploi{X^2}{K_{r,m}}-\esploi{X}{K_{r,m}}^2=
%    \esploi{\frac{a^2+ab+b^2}{3}}{K_{r,m}}-\left(\frac{2r-1}{2(m+1)}\right)^2
%.\end{gather*}
%We write~$a^2+ab+b^2=(-(a-b)^2+3(a^2+b^2))/2$.\\
%So:
%\begin{gather*}
%  \esploi{a^2}{K_{r,m}}=\frac{r(r-1)}{(m+1)(m+2)}\\
%  \esploi{b^2}{K_{r,m}}=\frac{(r+1)r}{(m+1)(m+2)}\\
%  \esploi{(b-a)^2}{K_{r,m}}=\frac{m!}{(r-2)^+!(m-r)^+!} \times \int_{0<u<k<v<1}{ u^{(r-2)^+}(v-u)^2(1-v)^{(m-r)^+}dudv}=\\
%    \frac{2}{(m+1)(m+2)}\\
%.\end{gather*}
%It makes:
%\begin{gather*}
%  \esploi{\frac{a^2+ab+b^2}{3}}{K_{r,m}}=\frac{3r^2-1}{3(m+1)(m+2)}
%,\end{gather*}
%which gives the result.
%factor((3*r^2-1)/(3*(m+1)*(m+2))-((2*r-1)/(2*(m+1)))^2);
\begin{gather*}
  \esploi{X}{K_{r,m}}=\frac{2r-1}{2(m+1)}\simeq \frac{r}{m+1}\\
  \varloi{X}{K_{r,m}}=\frac{12r(m-r)+24r-7m-10}{12(m+1)^2(m+2)}\simeq \frac{r(m-r)}{(m+1)^2(m+2)}% \leq (4m)^{-1}
.\end{gather*}

\begin{theorem}\label{equivkernel} Assuming that~$\mathbf{X}=(X_1, \cdots, X_d)$ in~$\mathbb{R}^d$ has continuous marginals, and copula density~$c$ is bounded by~$M$, we have:
\begin{gather*}
   m^d \times \left| P(m,\mathbf{r},c)-P_K(m,\mathbf{r},c) \right|\leq \frac{2M^2d^2}{m} + O(m^{-2})
,\end{gather*}
with:
\begin{gather*}
  P_K(m,\mathbf{r},c)=\int_{[0,1]^d}{\left(\prod_{l=1}^d{K_{r_l,m-1}(u_l)}\right)c(\mathbf{u})d\mathbf{u}}
.\end{gather*}
\end{theorem}

\section{Convergence speed}
In the independent case, one shows that the AMISE is the same as for a kernel estimation. For more details, see~\ref{casindep}.

\subsection{Use for independence test}\label{testtest}
We propose here to use independence testing to measure the accuracy of our estimation technique.\\
To build a test, we need a test statistic: we use the Kullback--Leibler divergence, between~$\hat{P}$ and the asymptotic value in case of independence, it means $P \equiv m^{-d}$. Then, we have to simulate many samples with independent components, which gives the distribution of the test statistic. Finally, we simulate samples with dependent components, and count the number of independence rejections.\\
In order to study the behavior of this testing method when the dimension increases, we will use as a test case a given dependence between a given number of components, all other components being independent. We studied three cases.\\
The first two are described by the same equation, with a parameter~$p$ switching from a monotonic dependence if equal to~1 to a non-monotonic if equal to~2:
\begin{gather*}
\begin{array}{lll}
  X_2=a\cdot X_1^p + \epsilon & &\\
  X_i \sim \mathcal{N}(0,1) & \textrm{ if } & i \neq 2\\
  \epsilon \sim \mathcal{N}(0,1) &  &\\
\end{array}
.\end{gather*}
The third dependence is the following model:
\begin{gather*}
\begin{array}{lll}
  V \sim \mathcal{LN}(0,a)&&\\
  X_i \sim \mathcal{N}(0,V) & \textrm{ if } & i \leq d_d\\
  X_i \sim \mathcal{N}(0,1) & \textrm{ if } & d_d< i \leq d
\end{array}
\end{gather*}
where~$\mathcal{LN}$ denotes the log-normal distribution. In other words, the first~$d_d$ components have the same random volatility. If $a$~is large, the dependence is strong, and null if~$a=0$.\\
The main insight of this simulation study is: in order to maintain the same test power, it is enough to increase the sample size linearly w.r.t. dimension. Indeed, the sample size is here $15\times d$, so it is 30 for dimension~2, \dots, and 90 for dimension~6: this increase is enough to obtain good power for each dimension.\\
An additional point is the test power is never smaller than 1.1 times the power of the Deheuvels test, which is a widely used independence test. More precisely, The Deheuvels test shows a good power on the linear dependence, because it is a monotonic dependence.  On others, its power is around 0.05, so the test is useless.
\begin{figure}
\begin{center}
  \includegraphics[width=\taillecarre, height=\taillecarre]{despowers.pdf}
\caption{\label{plotpower}\textbf{Example of use for independence testing}: the sample size increases linearly with the dimension, so is equal to~$15\times d$. The sub-sample size is~8. The thin interrupted line represents the 5\%~level. The linear increasing of sample size is enough to obtain good power for each dimension.}
\end{center}
\end{figure}


\subsection{Forecasting competition} We used this method to participate in a forecasting competition~\cite{Gefcom}, with good results: the model, yet simple, finished tenth in the competition, among 250~competitors, with a forecast error 10\% larger than the winner.\\
The topic of the probabilistic wind power forecasting track was to forecast the probabilistic distribution (in quantiles) of the wind power generation for 10~wind farms on a rolling basis. The target variable was power generation, normalized here by the respective nominal capacities of each wind farm. The explanatory variables that can be used are the past power measurements (if relevant), and input weather forecasts, given as~$u$ and~$v$ components (zonal and meridional). These forecasts were given at two heights, 10~m and 100~m above ground level, in order to give a rough idea of wind profiles.\\
Since it is possible to estimate directly the density for dimensions at around~6, we only had to estimate the joint density of the load factor~$\textit{LF}$, and the 4~wind speeds $u_{10}$, $v_{10}$, $u_{100}$, $v_{100}$. In a second step, we simulated 1000~values of~$\textit{fc}$, knowing the values of the other variables.\\
This simple first model was a bit modified, because some temporal smoothing improved the results. In the final model, the 4~wind speeds are smoothed, with a window size equal to~5 time steps, and a set of weights derived from the triweight kernel. It appeared also useful to take into account the variability of the wind. To do so, a first step is to compute the absolute speed of the wind at each level. Then, one computes the local sum of squares of the speed, and it is added to the regressors of the load factor.


\bibliographystyle{plainnat}
\bibliography{bibcop}{}

\appendix

\section{Proof of theorem~\ref{equivkernel}}
Without loss of generality, we assume the random variable~$\mathbf{X}$ has uniform marginals. We will compute~$P(m,\mathbf{r},c)$, conditioning by: the rank~$\mathbf{r}$ is reached by the last observation. Using symmetry, it would be the same for the $m$~other observations. In order for the last observation to reach rank~$\mathbf{r}$, one needs its first coordinate to be between~$X_{1,(r_1-1,m-1)}$ and~$X_{1,(r_1,m-1)}$, where~$X_{1,(r_1-1,m-1)}$ denotes the value with rank~$r_1-1$ among~$m-1$, for the first coordinate; and it is the same for other coordinates, so we get\\
\begin{gather}\label{trou}
P(m,\mathbf{r},c) =
\esp{ \int_{ \bigcap{[X_{l,(r_l-1,m-1)};X_{l,(r_l,m-1)}]} }{c(\mathbf{u})d\mathbf{u}} }
.\end{gather}
We know that on each dimension, order statistics are Beta distributed. Then, if we have independence, the equality with~$P_K$ is obvious. If, for any couples~$(l,l')$ and~$(\epsilon_l,\epsilon_{l'})$ ($\epsilon_l \in \{0,1\}$), the values~$X_{1,(r_1-\epsilon_l,m-1)}$ and~$X_{1',(r_{l'}-\epsilon_{l'},m-1)}$ derive from different observations, they are independent and equality with~$P_K$ is true.\\
So we study the probability of coincidence: some of the values~$X_{1,(r_1-\epsilon_l,m-1)}$ and~$X_{1',(r_{l'}-\epsilon_{l'},m-1)}$ derive from the same observation. For a given couple~$(l,l')$, we assume that~$X_{l,(r_l-1,m-1)}$ is the value of observation~1 (without loss of generality). We want to know the probability that ~$X_{l',(r_l'-1,m-1)}$ is the value of observation~1 as well. To compute this probability, we have to consider the $m-2$~remaining observations:
\begin{gather*}
  \proba{X_{l',1} \in [X_{l',(r_{l'}-1,m-2)},X_{l',(r_{l'},m-2)}]} \leq M \times \esp{X_{l',(r_{l'},m-2)}-X_{l',(r_{l'}-1,m-1)}}= \frac{M}{m-2}
.\end{gather*}
If the couple~$(l,l')$ is not given, we know that, for each dimension, $X_{1,(r_1,m-1)}$ and~$X_{1,(r_1-1,m-1)}$ are obviously from different observations. So we have to choose 2~dimensions, and for each the value of~$\epsilon_l$, so the probability of a coincidence is less than~$2d(d-1)\times M/(m-2)$.\\
We need the following result, inspired from~\cite{borneFrechet}:
\begin{proposition}\label{Touzi}Let $\phi$ be a superadditive function, $\mathbf{X}$ a $d$-dimensional random variables with marginals~$F_i$, $\Gamma_F$ the set of the distributions with marginals~$F_i$. Assuming~$\phi(\mathbf{X})$ is uniformly integrable on~$\Gamma_F$, we know, for~$H \in \Gamma_F$
\begin{gather*}
  \esploi{H}{\phi(\mathbf{X})} \leq \esploi{\bar{H}}{\phi(\mathbf{X})} 
,\end{gather*}
where~$\bar{H}$ is the distribution with marginals~$F_i$, and copula equal to the upper Fr\'echet-Hoeffding bound.
\end{proposition}
If we have $e$~coincidences, for example on dimensions~1,$\cdots,e$, we have to bracket $$\esp{ \prod_{l=1}^d{\left(X_{l,(r_l,m-1)}-X_{l,(r_l-1,m-1)}\right)}}.$$
A lower-bound is obviously~0, so we focus on upper-bound. We know:
\begin{gather*}
  \esp{ \prod_{l=1}^d{\left(X_{l,(r_l,m-1)}-X_{l,(r_l-1,m-1)}\right)}}=m^{-(d-e)}\esp{ \prod_{l=1}^e{\left(X_{l,(r_l,m-1)}-X_{l,(r_l-1,m-1)}\right)}}
.\end{gather*}
In the product in the expectation of r.h.s., we know the distribution of each term, since it is the difference of 2~consecutive order statistics, but we do not know the joint distribution. Using~\ref{Touzi}, we know the worst case is the perfect correlation. So we have
\begin{gather*}
  \esp{ \prod_{l=1}^e{\left(X_{l,(r_l,m-1)}-X_{l,(r_l-1,m-1)}\right)}} \leq \esp{ \left(X_{1,(r_1,m-1)}-X_{1,(r_1-1,m-1)}\right)^e}=\\
  \frac{e!m!}{(m+e)!} \leq e! m^{-e}
,\end{gather*}
which gives
\begin{gather*}
  \esp{ \prod_{l=1}^d{\left(X_{l,(r_l,m-1)}-X_{l,(r_l-1,m-1)}\right)}} \leq e! m^{-d}
.\end{gather*}
To provide a global bound on~$\left| P(m,\mathbf{r},c)-P_K(m,\mathbf{r},c) \right|$, we address only two cases: 1 coincidence, and more than~1. The probability of the first case has been computed previously.\\
For the latter, we know the number of choices of dimension~$l$, interval side~$\epsilon_l$ is finite, and does not depend on~$m$. The probability of more than 1~coincidence, given the  choice of dimensions~$l$ and interval sides~$\epsilon_l$ is less than~$(M/(m-2))^2$. For each coincidence, the contribution to the value of $\esp{ \prod_{l=1}^d{\left(X_{l,(r_l,m-1)}-X_{l,(r_l-1,m-1)}\right)}}$ is less than~$d! m^{-d}$. So the total contribution is of order~$O(m^{-2})$.\\
Then last step is taking into account the value of copula~$c$, which is bounded by~$M$. We finally get:
\begin{gather*}
  m^d \times \left| P(m,\mathbf{r},c)-P_K(m,\mathbf{r},c) \right|\leq \frac{2M^2d^2}{m} + O(m^{-2})
.\end{gather*}

\section{Independent case study}\label{casindep}
In the following, we will study
\begin{gather*}
 \hat{T}_n(m,\mathbf{X}) = m^d \times \sum_{\mathbf{r}}{\left(\hat{P}_n(m,\mathbf{r},\mathbf{X}) - m^{-d} \right)^2}
,\end{gather*}
where components of~$\mathbf{X}$ are globally independent. More precisely, we study~$\var{\hat{T}_n(m,\mathbf{X})}$.
\begin{theorem}\label{varnul}
Assuming that~$\mathbf{X}=(X_1, \cdots, X_d)$ in~$\mathbb{R}^d$ has continuous marginals, if the components are globally independent and if the sample size~$n$ tends to infinity,
\begin{gather*}
\begin{array}{lll}
 n\esp{\hat{T}_n(m,\mathbf{X})} &\rightarrow &
     S_1^d -m^2 + (m-1)^2 \left(\frac{m^2-2m+S_1}{(m-1)^2}\right)^d+2(m-1)\left(\frac{m-S_1}{m-1}\right)^d \\
 n^2\var{\hat{T}_n(m,\mathbf{X})} & \rightarrow &
    2S_2^d - 2m^4+\\
    & &2(m-1)^4 \left(\frac{m^4-4m^3+6m^2-4m+S_2}{(m-1)^4}\right)^{d}+
    12(m-1)^2 \left(\frac{m^2-2m+S_2}{(m-1)^2}\right)^{d}+\\
    & & 8(m-1) \left(\frac{m-S_2}{m-1}\right)^{d} +
    8(m-1)^3 \left(\frac{m^3-3m^2+3m-S_2}{(m-1)^3}\right)^{d}
\end{array}
\end{gather*}
with
\begin{gather*}
\begin{array}{ll}
 S_1=\frac{m4^{m-1}}{(2m-1)\binom{2m-2}{m-1}}
&
 S_2=\frac{m^2\binom{4m-3}{2m-2}}{\left((2m-1)\binom{2m-2}{m-1}\right)^2}
\end{array}
.\end{gather*}
\end{theorem}
A numerical study shows that the first terms are the most important. Furthermore, it is possible to approximate these first terms using Stirling's formula. So we have
\begin{gather*}
\begin{array}{ll}
  \var{\hat{T}_n(m,\mathbf{X})} \simeq  2n^{-2}\left(\sqrt{\frac{\pi m}{8}}\right)^d
  &
  \esp{\hat{T}_n(m,\mathbf{X})} \simeq  n^{-1}\left(\frac{\sqrt{\pi m}}{2}\right)^d
\end{array}
.\end{gather*}
We can check again the equivalence with Kendall's~$\tau$. We show~$\hat{T}_n(2,\mathbf{X})=\tau^2$, so we check that~$\esp{\hat{T}_n(2,\mathbf{X})}\simeq\var{\tau}$ when~$n$ tends to infinity.
% Juste pour tracer, je sais pas ou le mettre sinon. \\ We know that~$ \hat{\tau}=2(\hat{P}(2,(1,1),\mathbf{X})-\hat{P}(2,(1,2),\mathbf{X}))$. With the constraints on sums of rows or columns, we have~$\hat{P}(2,(1,1),\mathbf{X})=\hat{P}(2,(2,2),\mathbf{X})=(1+\tau)/4$ and~$\hat{P}(2,(1,2),\mathbf{X})=\hat{P}(2,(2,1),\mathbf{X})=(1-\tau)/4$; so~$\hat{T}_n(2,\mathbf{X})=\tau^2$. It is well known that~$\var{\tau}=2(2n+5)/(9n(n-1))\simeq 4/9n$, it is what we get with~$m=d=2$ in expression of~$\esploi{\hat{T}_n(2,\mathbf{X})}{c_0}$.
\subsection{Numerical verification}\label{verifvar}
We study by simulation the convergence of true value towards theoretical value, for mean and variance of~$\hat{T}_n(m,\mathbf{X})$, where components of~$\mathbf{X}$ are independent. We choose a sub-sample size equal to~10 and for each dimension in~$\{2,3,4,5\}$, we plot in figure~\ref{plotconv} the absolute relative error of the theoretical ~$|\textrm{true}/\textrm{theoretical}-1|$: it decreases rapidly when the sample size is between~30 and~3000. For all simulations, we choose a number of sub-samples equal to~$n\times m^d$, and a number of samples equal to~100.\\
\begin{figure}
\begin{center}\begin{tabular}{cc}
Mean of~$\hat{T}$ & Variance of~$\hat{T}$  \\
  \includegraphics[width=\taillecarre, height=\taillecarre]{convEsp.pdf} &
  \includegraphics[width=\taillecarre, height=\taillecarre]{convVar.pdf}
\end{tabular}
\caption{\label{plotconv}\textbf{Convergence of true value towards theoretical value}: for mean and variance of~$\hat{T}(m,\mathbf{X})$, where components of~$\mathbf{X}$ are independent, we study the convergence to~0 of absolute relative error~$|\textrm{true}/\textrm{theoretical}-1|$. Sub-sample size is~10, scale is logarithmic on both axes. The convergence becomes slower when dimension increases.}
\end{center}
\end{figure}

\subsection{Evaluation of AMISE in the independent case}
In the asymptotic case, this estimation behaves as a kernel estimation, with bandwidth of order~$m^{-1/2}$. In the independent case, the variance is of order~$m^{d/2}$.\\
So, \important{in the independent case}, the AMISE is the same as  for a kernel estimation.\\
We think, without proof, that this case is the worst case for the method we propose. Indeed, with independence, the exclusion constraints (exactly 1~observation on each hyperplane of~$\ens{m}^d$), are not very important. That is why we think in other cases it would show better performances than the kernel estimation. The simulation study corroborates this intuition.\\
Furthermore, if we use the optimal value of~$m$, to compute~$T$ and its variance, then the convergence speed is of order~$n^{(-d-8)/(d+4)}$, which makes it useful for independence testing.


\subsection{Proof of Theorem~\ref{varnul}}
Addressing the behaviour of~$\hat{P}_n(m,\mathbf{r},\mathbf{X})$ is obviously a bit more difficult than addressing Kendall's~$\tau$, which is the case~$m=d=2$. Now, as far as we know, there is no simple formula for the probability distribution of Kendall's~$\tau$: neither ~\cite{wolfe1973nonparametric}, nor the~\cod{cor.test} function documentation (in the package~\cod{stats}) in \cod{R}, nor in the package~\cod{Kendall} of the same software, nor the \cod{SAS} documentation mention such a formula. There exist recursive ones in~\cite{BestGipps,valz1994exact}, which can be used only if the sample is small.\\
That is why we will derive here equivalents of the first two moments of~$T$, for large values of the sample size.
\subsubsection{The \textit{U}-statistics: reminders and notations}
Let $h$~be a measurable function, symmetric in its $n$~arguments. Then if we have a sample~$X_1, \cdots, X_N$ with~$n>m$, we define the $U$-statistic~$U_m$:
\begin{gather*}
  U_n = \frac{1}{\binom{n}{m}} \sum_{S \subset \ens{N}}^{\card{S}=m}
  {h\left(X_{S(1)},\cdots,X_{S(m)}\right)}
.\end{gather*}
where~$S(i)$ denotes the \ordinal{i} element of~$S$.\\
Furthermore, we define
\begin{gather*}
  h_c = \esp{h\left(x_1, \cdots, x_c,X_{c+1}, \cdots, X_m\right)}
,\end{gather*}
and
\begin{gather*}
  \sigma_c = \var{h_c\left(X_1, \cdots, X_c\right)}
.\end{gather*}
Then~$\esp{U_m}=h_0$ and, when~$n \rightarrow +\infty$, $n(U_m-h_0)$ converges in distribution to~$\mathcal{N}\left(0,m^2\sigma_1\right)$ if~$\sigma_1 \neq 0$.\\
Furthermore, if we have another $U$-statistic~$V_n$ defined by a kernel~$g$, we may also define~$g_c$ and~$\sigma_{c,c}$:
\begin{gather*}
  \sigma_{c,c} = \cov{h_c\left(X_1, \cdots, X_c\right),g_c\left(X_1, \cdots, X_c\right)}
.\end{gather*}
The covariance between~$U_n$ and~$V_n$ converges to~$\sigma_{1,1}$ when~$N \rightarrow +\infty$.\\
In the following, for each~$\mathbf{r}$, we have a $U$-statistic~$\hat{P}_n(m,\mathbf{r},\mathbf{X})$, whose normal convergence we will use.\\
As we are in a slightly special case of $U$-statistics (we are only interested in the case~$c=1$, but we study a large number of $U$-statistics at the same time), one has to adapt the notations. We note:
\begin{gather*}
  h(\mathbf{r},x_1)=\esp{B_\mathbf{r}\left(x_1,X_2\cdots,X_n\right)}\\
  \sigma(\mathbf{r},\mathbf{s})=\cov{h(\mathbf{r},X_1),h(\mathbf{s},X_1)}
.\end{gather*}
So we obtain
\begin{proposition}\label{hajekloc}
If~$n \rightarrow \infty$, one has
\begin{itemize}
  \item $n\left(\hat{P}_n(m,\mathbf{r},\mathbf{X})-P(m,\mathbf{r},c))\right)$ converges in distribution to~$\mathcal{N}\left(0,m^2\sigma(\mathbf{r},\mathbf{r})\right)$; if~$\sigma(\mathbf{r},\mathbf{r}) \neq 0$,
  \item $n\cov{\hat{P}_n(m,\mathbf{r}),\hat{P}_n(m,\mathbf{s},\mathbf{X})} \rightarrow m^2\sigma(\mathbf{r},\mathbf{s})$.
\end{itemize}
\end{proposition}
This Central Limit Theorem allows us to use the following computation:
\begin{proposition}\label{varD}
Let $\mathbf{X}$ be a vector such that$\mathbf{X} \leadsto \mathcal{N}(0,V)$, where the coefficients of~$V$ are denoted~$\sigma(\mathbf{r},\mathbf{s})$. We study~$D=\sum_{\mathbf{r}}{X_{\mathbf{r}}^2}$. Then
\begin{gather*}
  \esp{D}=\sum_{\mathbf{r}}{\sigma(\mathbf{r},\mathbf{r})}\ \ \ \ \ 
  \var{D}=2\sum_{\mathbf{r},\mathbf{s}}{ \sigma(\mathbf{r},\mathbf{s})^2 }
.\end{gather*}
\end{proposition}

\subsubsection{Calculation of the individual covariances}
We calculate in the same way the variances and covariances. We consider the first observation, its vector of ranks, and a given rank~$\mathbf{r}$. There are 3~cases:
\begin{itemize}
  \item Other observations are such that the vector of ranks of the first observation is~$\mathbf{r}$,
  \item Other observations are such that the vector of ranks of the first observation is equal to~$\mathbf{r}$ for some dimensions,
  \item Other observations are such that the vector of ranks of the first observation is different from~$\mathbf{r}$ for all dimensions.
\end{itemize}
We know the probability that the \ordinal{l} coordinate of the first observation reaches rank~$r_l$:
\begin{gather*}
  P=\binom{m-1}{r_l-1} x_{1,l}^{r_l-1}(1-x_{1,l})^{m-r_l}=b_{m-1,r_l-1}(x_{1,l})
,\end{gather*}
where $b_{m-1,r_l-1}$ is a Bernstein polynomial, with well-known properties, for example
\begin{gather*}
  \int_0^1{b_{m,r}(x)dx}=\frac{1}{m+1}
.\end{gather*}
We use this to calculate the probability of each one of the three cases:
\begin{gather*}
  \proba{B_\mathbf{r}\left(x_1,X_2\cdots,X_n\right)=\frac{1}{m}}=\\
  1 \times \prod_{l=1}^d{b_{m-1,r_l-1}(x_{1,l})}+
  0\times\left(1-\prod_{l=1}^d{\left(1-b_{m-1,r_l-1}(x_{1,l})\right)}-\prod_{l=1}^d{b_{m-1,r_l-1}(x_{1,l})}\right)+\\
  \frac{m-1}{(m-1)^d}\times\prod_{l=1}^d{\left(1-b_{m-1,r_l-1}(x_{1,l})\right)}
.\end{gather*}
One can remark that integrating this probability over~$\mathbf{x}_1$ gives back the unconditional probability
\begin{gather*}
  \proba{B_\mathbf{r}\left(X_1,X_2\cdots,X_n\right)=\frac{1}{m}}=\\
  1 \times \int_{[0,1]^d}{\prod_{l=1}^d{b_{m-1,r_l-1}(x_{1,l})}d\mathbf{x}_1}+
  \frac{1}{(m-1)^{d-1}}\times\int_{[0,1]^d}{\prod_{l=1}^d{\left(1-b_{m-1,r_l-1}(x_{1,l})\right)}d\mathbf{x}_1}=\\
  1\times\frac{1}{m^d}+\frac{1}{(m-1)^{d-1}}\times\left(1-\frac{1}{m}\right)^d=\frac{1}{m^{d-1}}
.\end{gather*}
The conditional mean is
\begin{gather*}
  h(\mathbf{r},\mathbf{x}_1)=\frac{1}{m}\esp{B_\mathbf{r}\left(\mathbf{x}_1,X_2\cdots,X_n\right)}=\\
  \frac{1}{m} \times
  \left(
    \prod_{l=1}^d{b_{m-1,r_l-1}(x_{1,l})}+
    \frac{1}{m(m-1)^{d-1}}\times\prod_{l=1}^d{\left(1-b_{m-1,r_l-1}(x_{1,l})\right)}
  \right)
.\end{gather*}
So, we have to calculate
\begin{gather*}
  \sigma(\mathbf{r},\mathbf{s})=\esp{
  \left[h(\mathbf{r},\mathbf{X}_1) - \esp{h(\mathbf{r},\mathbf{X}_1)} \right] \times  \left[h(\mathbf{s},\mathbf{X}_1) - \esp{h(\mathbf{s},\mathbf{X}_1)} \right]
    } = \\
  \int_{[0,1]^d}{
  \begin{array}{c}
  \left[
  \frac{1}{m}  \prod_{l=1}^d{b_{m-1,r_l-1}(x_{1,l})}+
  \frac{1}{m(m-1)^{d-1}}\prod_{l=1}^d{\left(1-b_{m-1,r_l-1}(x_{1,l})\right)}
  -\frac{1}{m^d}
  \right] \times \\
  \left[
  \frac{1}{m}  \prod_{l=1}^d{b_{m-1,s_l-1}(x_{1,l})}+
  \frac{1}{m(m-1)^{d-1}}\prod_{l=1}^d{\left(1-b_{m-1,s_l-1}(x_{1,l})\right)}
  -\frac{1}{m^d}
  \right] 
  \end{array} d\mathbf{x}_1}
.\end{gather*}
For these covariance computations, we will calculate expressions such as
\begin{gather*}
  \int_0^1{b_{m,r}(x)b_{m,s}(x)dx}=
  \frac{\binom{m}{r}\binom{m}{s}}{\binom{2m}{r+s}}\times\frac{1}{2m+1}
.\end{gather*}
We note
\begin{gather*}
  \mathcal{A}(m,r,s)=\frac{\binom{m}{r}\binom{m}{s}}{\binom{2m}{r+s}}
.\end{gather*}
We multiply two sums of three terms. We denote by~$D_{i,j}$ the product of terms numbered~$i$ in the first sum and~$j$ in the second one. 
\begin{gather*}
\begin{array}{lll}
  D_{1,1}&=&\int_{[0,1]^d}{\left[
    \frac{1}{m} \times \prod_{l=1}^d{b_{m-1,r_l-1}(x_{1,l})b_{m-1,s_l-1}(x_{1,l})}
  \right]dx}\\
  &=&
  \frac{1}{m^2(2m-1)^{d}} \times \prod_{l=1}^d{\mathcal{A}(m-1,r_l-1,s_l-1)}\\
  D_{2,2}&=&\int_{[0,1]^d}{\left[
    \frac{1}{m^2(m-1)^{2d-2}} \times\prod_{l=1}^d{\left((1-b_{m-1,r_l-1}(x_{1,l}))(1-b_{m-1,s_l-1}(x_{1,l}))\right)}
  \right] dx}\\
  &=&
  \frac{1}{m^2(m-1)^{2d-2}}\times\prod_{l=1}^d{\left(1-\frac{2}{m}+\frac{\mathcal{A}(m-1,r_l-1,s_l-1)}{2m-1}\right)}\\
  D_{3,3}&=&\int_{[0,1]^d}{\left[\frac{1}{m^{d}}\right]^2 dx}\\
  &=&\frac{1}{m^{2d}}\\
  D_{1,2}&=&\int_{[0,1]^d}{
    \frac{1}{m^2(m-1)^{d-1}} \times
    \prod_{l=1}^d{\left(b_{m-1,r_l-1}(x_{1,l})-b_{m-1,r_l-1}(x_{1,l})b_{m-1,s_l-1}(x_{1,l})\right)}
  dx}\\
  &=&\frac{1}{m^2(m-1)^{d-1}} \times \prod_{l=1}^d{\left(\frac{1}{m}-\frac{\mathcal{A}(m-1,r_l-1,s_l-1)}{2m-1}\right)}\\
  D_{1,3}&=&\int_{[0,1]^d}{\left[
    \frac{1}{m} \times \prod_{l=1}^d{b_{m-1,r_l-1}(x_{1,l})} \times \frac{1}{m^d}
  \right]dx}\\
   &=&\frac{1}{m^{2d+1}}\\
  D_{2,3}&=&2\int_{[0,1]^d}{\left[
    \frac{1}{m^{d+1}(m-1)^{d-1}} \times\prod_{l=1}^d{\left(1-b_{m-1,r_l-1}(x_{1,l})\right)}
  \right]dx}\\
  &=&\frac{m-1}{m^{2d+1}}
\end{array}
\end{gather*}
It is clear that~$D_{1,2}=D_{2,1}$, $D_{1,3}=D_{3,1}$ and~$D_{2,3}=D_{3,2}$, so we write simply~$2D_{1,2}$, etc.\\
We remark that
\begin{gather*}
  D_{3,3}-2D_{1,3}-2D_{2,3}=-m^{-2d}.
\end{gather*}

\subsubsection{Calculation of the sum of the covariances}
We know
\begin{gather*}
 \sigma(\mathbf{r},\mathbf{s})=D_{1,1}+D_{2,2}+D_{3,3}+2D_{1,2}-2D_{1,3}-2D_{2,3}=\\
 D_{1,1}+D_{2,2}+2D_{1,2}-m^{-2d}
,\end{gather*}
so
\begin{gather*}
 \esp{\hat{T}_n(m,\mathbf{X})}=\sum_{\mathbf{r}}{\sigma(\mathbf{r},\mathbf{r})}=
 \sum_{\mathbf{r}}{\left(D_{1,1}(\mathbf{r},\mathbf{r})+D_{2,2}(\mathbf{r},\mathbf{r})+2D_{1,2}(\mathbf{r},\mathbf{r})\right)}-m^{-d}
.\end{gather*}
In a similar way
\begin{gather*}
 \var{\hat{T}_n(m,\mathbf{X})}=2\sum_{\mathbf{r},\mathbf{s}}{\sigma^2(\mathbf{r},\mathbf{s})}=
 2\sum_{\mathbf{r},\mathbf{s}}{\left(D_{1,1}(\mathbf{r},\mathbf{s})+D_{2,2}(\mathbf{r},\mathbf{s})+2D_{1,2}(\mathbf{r},\mathbf{s})-m^{-2d}\right)^2}=\\
 2\sum_{\mathbf{r},\mathbf{s}}{\left(
 \begin{array}{l}
  D^2_{1,1}(\mathbf{r},\mathbf{s})+D^2_{2,2}(\mathbf{r},\mathbf{s})+4D^2_{1,2}(\mathbf{r},\mathbf{s})+m^{-4d}\\
 +2D_{1,1}D_{2,2}+4D_{1,1}D_{1,2}+4D_{2,2}D_{1,2}\\
 -2m^{-2d}\left[D_{1,1}(\mathbf{r},\mathbf{s})+D_{2,2}(\mathbf{r},\mathbf{s})+2D_{1,2}(\mathbf{r},\mathbf{s})\right]
 \end{array}
 \right)}=\\
 2\sum_{\mathbf{r},\mathbf{s}}{\left(
  D^2_{1,1}(\mathbf{r},\mathbf{s})+D^2_{2,2}(\mathbf{r},\mathbf{s})+6D^2_{1,2}(\mathbf{r},\mathbf{s})
 +4D_{1,1}D_{1,2}+4D_{2,2}D_{1,2}
 \right)} -2m^{-2d}
.\end{gather*}
We will need to know some sums involving~$\mathcal{A}(m,r,s)$, they are proved in~\ref{calcomb}.
\begin{gather*}
 \sum_{r}{\mathcal{A}(m,r,s)}=\frac{2m+1}{m+1},\ \ \ 
 \sum_{r}{\mathcal{A}(m,r,r)}=\frac{4^m}{\binom{2m}{m}},\ \ \ 
 \sum_{r,s}{\left(\mathcal{A}(m,r,s)\right)^2}=\frac{\binom{4m+1}{2m}}{(\binom{2m}{m})^2}
\end{gather*}
The sums of the terms~$D_{i,j}$ are sums of products, we transform them easily into products of sums. For example
\begin{gather*}
  \sum_{\mathbf{r},\mathbf{s}}{D_{1,1}^2} = 
  \frac{1}{m^4(2m-1)^{2d}} \times
    \sum_{\mathbf{r},\mathbf{s}}\left(\prod_{l=1}^d{\mathcal{A}^2(m-1,r_l-1,s_l-1)}\right)=\\
  \frac{1}{m^4(2m-1)^{2d}} \times
    \left(\sum_{r,s}\mathcal{A}^2(m-1,r-1,s-1)\right)^d
.\end{gather*}
All of these sums (3~sums of degree~1 with~$\mathbf{r}=\mathbf{s}$, 3~sums of degree~1, 3 sums of squares, and 3~sums of double products) are calculated in~\ref{somD}. Using these sums, the proof of Theorem~\ref{varnul} is obvious.

\subsection{Tools for the proof of Theorem~\ref{varnul}}
\subsubsection{Combinatorial computations}\label{calcomb}
We need to compute some sums involving~$\mathcal{A}(m,r,s)$. We first remark:
\begin{gather*}
  \mathcal{A}(m,r,s)=\frac{\binom{m}{r}\binom{m}{s}}{\binom{2m}{r+s}}=\frac{\binom{r+s}{r}\binom{2m-r-s}{m-r}}{\binom{2m}{m}}
.\end{gather*}
We show
\begin{proposition}{If~$m>1$ and~$0\leq s \leq m$:
\begin{gather*}
 \sum_{r}{\binom{r+s}{r}\binom{2m-r-s}{m-r}}=\binom{2m+1}{m}\\
 \sum_{r}{\binom{2r}{r}\binom{2m-2r}{m-r}}=4^m\\
 \sum_{r,s}{\left(\binom{r+s}{r}\binom{2m-r-s}{m-r}\right)^2}=\binom{4m+1}{2m}
.\end{gather*}
}
\end{proposition}
These sums are very similar to convolutions, so it is interesting to use generating functions~\citep{gfology}. They are quite simple for the first series:
\begin{gather*}
  \sum_n{\binom{n+k}{n}x^n}=\frac{1}{(1-x)^{k+1}}\\
  \sum_n{\binom{2n}{n}x^n}=\frac{1}{\sqrt{1-4x}}
.\end{gather*}
It is also possible to demonstrate the first identity using combinatorial arguments, counting the number of paths joining two opposite corners of a rectangle with sides~$m$ and~$m+1$. The last identity is more difficult: one needs to use the powerful tools developed in~\cite{aeqb}. As the sum is over 2~variables, one needs to use the package~\cod{multisum}~\citep{multisum}. The code proving identity is
\begin{verbatim}
Get["C:\Users\Jerome\Desktop\Celine\MultiSum.m"]
FindRecurrence[ (Binomial[r+s,r]*Binomial[2*m-r-s,m-r])^2,
  m, {r,s}, 4 ]
SumCertificate[%]
CheckRecurrence[ %, Binomial[4*m+1,2*m] ]
\end{verbatim}

\subsubsection{Sums of covariance terms}\label{somD}
We summarize here the sums of terms~$D_{i,j}$. These sums are all computed in the same way, and they are compulsory to check the other computations.
\begin{gather*}
\begin{array}{lll}
  \sum_{\mathbf{r}}{D_{1,1}(\mathbf{r},\mathbf{r})} &=
    & \frac{1}{m^{2}} \times R_1^d\\
  \sum_{\mathbf{r}}{D_{2,2}(\mathbf{r},\mathbf{r})} &=
    & \left(\frac{m-1}{m}\right)^2 \times \left(\frac{m-2+R_1}{(m-1)^2}\right)^d\\
  \sum_{\mathbf{r}}{D_{1,2}(\mathbf{r},\mathbf{r})} &=
    & \frac{m-1}{m^{2}}\times \left(\frac{1-R_1}{m-1}\right)^d\\
  \sum_{\mathbf{r},\mathbf{s}}{D_{1,1}} &=
    &\frac{1}{m^2} \\
  \sum_{\mathbf{r},\mathbf{s}}{D_{2,2}} &=
    &\left(\frac{m-1}{m}\right)^{2} \\
  \sum_{\mathbf{r},\mathbf{s}}{D_{1,2}} &=
    &\frac{m-1}{m^2} \\
  \sum_{\mathbf{r},\mathbf{s}}{D_{1,1}^2} &=
    &\frac{1}{m^{4}} \times  R_2^d \\
  \sum_{\mathbf{r},\mathbf{s}}{D_{2,2}^2} &=
    &\left(\frac{m-1}{m}\right)^4 \times \left(\frac{m^2-4m+6-\frac{4}{m}+R_2}{(m-1)^4}\right)^{d} \\
  \sum_{\mathbf{r},\mathbf{s}}{D_{1,2}^2} &=
    &\frac{(m-1)^2}{m^4} \times \left(\frac{1-\frac{2}{m}+R_2}{(m-1)^2}\right)^{d} \\
  \sum_{\mathbf{r},\mathbf{s}}{D_{1,1}D_{2,2}}&= &\sum_{\mathbf{r},\mathbf{s}}{D_{1,2}^2} \\
  \sum_{\mathbf{r},\mathbf{s}}{D_{1,1}D_{1,2}} &=
    &\frac{m-1}{m^4} \times \left(\frac{\frac{1}{m}-R_2}{m-1}\right)^{d} \\
  \sum_{\mathbf{r},\mathbf{s}}{D_{2,2}D_{1,2}} &=
    &\frac{(m-1)^3}{m^4} \times \left(\frac{m-3+\frac{3}{m}-R_2}{(m-1)^3}\right)^{d} 
\end{array}
,\end{gather*}
with
\begin{gather*}
 R_2=\frac{\binom{4m-3}{2m-2}}{\left((2m-1)\binom{2m-2}{m-1}\right)^2}\ \ \ 
 R_1=\frac{4^{m-1}}{(2m-1)\binom{2m-2}{m-1}}
.\end{gather*}

\end{document}
